{
  "model_name": "roberta-base",
  "output_dir": "/home/christian.jesinghaus/FL/130126/MTL-LoRA/outputs/roberta_glue_mlora_23446",
  "seed": 42,
  "epochs": 1,
  "train_batch_size": 8,
  "eval_batch_size": 32,
  "grad_accum_steps": 2,
  "learning_rate": 0.0002,
  "warmup_ratio": 0.06,
  "max_length": 256,
  "num_workers": 2,
  "fp32": false,
  "lora_r": 8,
  "lora_alpha": 16,
  "lora_dropout": 0.05,
  "num_B": 3,
  "temperature": 0.1,
  "offline": false,
  "hf_token": null,
  "glue_disk_cache_dir": null,
  "hf_datasets_cache_dir": null,
  "save_every_updates": 500,
  "save_total_limit": 5,
  "resume_from": null,
  "eval_only": false
}