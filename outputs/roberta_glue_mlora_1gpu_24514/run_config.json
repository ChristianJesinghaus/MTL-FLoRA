{
  "model_name": "roberta-base",
  "output_dir": "/home/christian.jesinghaus/FL/130126/MTL-LoRA/outputs/roberta_glue_mlora_1gpu_24514",
  "seed": 42,
  "epochs": 1,
  "train_batch_size": 8,
  "eval_batch_size": 32,
  "grad_accum_steps": 2,
  "learning_rate": 0.0002,
  "warmup_ratio": 0.06,
  "max_length": 256,
  "num_workers": 2,
  "fp16": true,
  "lora_r": 8,
  "lora_alpha": 16,
  "lora_dropout": 0.05,
  "num_B": 3,
  "temperature": 0.1,
  "freeze_bias": true,
  "freeze_layernorm": true,
  "save_steps": 2500,
  "save_total_limit": 2,
  "save_pre_eval_ckpt": true,
  "resume_from_ckpt": null,
  "offline": false,
  "hf_token": null,
  "glue_disk_cache_dir": null,
  "hf_datasets_cache_dir": null,
  "skip_eval": false,
  "save_eval_details": true,
  "eval_details_max_examples": 200,
  "eval_details_only_errors": false,
  "eval_details_topk": 2,
  "stsb_abs_err_threshold": 0.5
}